% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{zareian2021open}
A.~Zareian, K.~D. Rosa, D.~H. Hu, and S.-F. Chang, ``Open-vocabulary object
  detection using captions,'' in \emph{Proceedings of the IEEE/CVF conference
  on computer vision and pattern recognition}, 2021, pp. 14\,393--14\,402.

\bibitem{scheirer2012toward}
W.~J. Scheirer, A.~de~Rezende~Rocha, A.~Sapkota, and T.~E. Boult, ``Toward open
  set recognition,'' \emph{IEEE transactions on pattern analysis and machine
  intelligence}, vol.~35, no.~7, pp. 1757--1772, 2012.

\bibitem{girshick2014rich}
R.~Girshick, J.~Donahue, T.~Darrell, and J.~Malik, ``Rich feature hierarchies
  for accurate object detection and semantic segmentation,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2014, pp. 580--587.

\bibitem{liu2016ssd}
W.~Liu, D.~Anguelov, D.~Erhan, C.~Szegedy, S.~Reed, C.-Y. Fu, and A.~C. Berg,
  ``Ssd: Single shot multibox detector,'' in \emph{Computer Vision--ECCV 2016:
  14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016,
  Proceedings, Part I 14}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
  2016, pp. 21--37.

\bibitem{redmon2016you}
J.~Redmon, S.~Divvala, R.~Girshick, and A.~Farhadi, ``You only look once:
  Unified real-time object detection,'' in \emph{Proceedings of the IEEE
  conference on computer vision and pattern recognition}, 2016, pp. 779--788.

\bibitem{akyon2022slicing}
F.~C. Akyon, S.~O. Altinuc, and A.~Temizel, ``Slicing aided hyper inference and
  fine-tuning for small object detection,'' in \emph{2022 IEEE international
  conference on image processing (ICIP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2022, pp. 966--970.

\bibitem{wu2022uiu}
X.~Wu, D.~Hong, and J.~Chanussot, ``Uiu-net: U-net in u-net for infrared small
  object detection,'' \emph{IEEE Transactions on Image Processing}, vol.~32,
  pp. 364--376, 2022.

\bibitem{yang2022querydet}
C.~Yang, Z.~Huang, and N.~Wang, ``Querydet: Cascaded sparse query for
  accelerating high-resolution small object detection,'' in \emph{Proceedings
  of the IEEE/CVF Conference on computer vision and pattern recognition}, 2022,
  pp. 13\,668--13\,677.

\bibitem{cheng2024yolo}
T.~Cheng, L.~Song, Y.~Ge, W.~Liu, X.~Wang, and Y.~Shan, ``Yolo-world: Real-time
  open-vocabulary object detection,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2024, pp.
  16\,901--16\,911.

\bibitem{liu2024grounding}
S.~Liu, Z.~Zeng, T.~Ren, F.~Li, H.~Zhang, J.~Yang, Q.~Jiang, C.~Li, J.~Yang,
  H.~Su \emph{et~al.}, ``Grounding dino: Marrying dino with grounded
  pre-training for open-set object detection,'' in \emph{European Conference on
  Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2024, pp.
  38--55.

\bibitem{ren2024grounding}
T.~Ren, Q.~Jiang, S.~Liu, Z.~Zeng, W.~Liu, H.~Gao, H.~Huang, Z.~Ma, X.~Jiang,
  Y.~Chen \emph{et~al.}, ``Grounding dino 1.5: Advance the" edge" of open-set
  object detection,'' \emph{arXiv preprint arXiv:2405.10300}, 2024.

\bibitem{girshick2015fast}
R.~Girshick, ``Fast r-cnn,'' in \emph{Proceedings of the IEEE international
  conference on computer vision}, 2015, pp. 1440--1448.

\bibitem{ren2016faster}
S.~Ren, K.~He, R.~Girshick, and J.~Sun, ``Faster r-cnn: Towards real-time
  object detection with region proposal networks,'' \emph{IEEE transactions on
  pattern analysis and machine intelligence}, vol.~39, no.~6, pp. 1137--1149,
  2016.

\bibitem{he2017mask}
K.~He, G.~Gkioxari, P.~Doll{\'a}r, and R.~Girshick, ``Mask r-cnn,'' in
  \emph{Proceedings of the IEEE international conference on computer vision},
  2017, pp. 2961--2969.

\bibitem{redmon2017yolo9000}
J.~Redmon and A.~Farhadi, ``Yolo9000: better, faster, stronger,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2017, pp. 7263--7271.

\bibitem{redmon2018yolov3}
------, ``Yolov3: An incremental improvement,'' \emph{arXiv preprint
  arXiv:1804.02767}, 2018.

\bibitem{bochkovskiy2020yolov4}
A.~Bochkovskiy, C.-Y. Wang, and H.-Y.~M. Liao, ``Yolov4: Optimal speed and
  accuracy of object detection,'' \emph{arXiv preprint arXiv:2004.10934}, 2020.

\bibitem{jocher2020ultralytics}
G.~Jocher, A.~Stoken, J.~Borovec, L.~Changyu, A.~Hogan, L.~Diaconu,
  J.~Poznanski, L.~Yu, P.~Rai, R.~Ferriday \emph{et~al.}, ``ultralytics/yolov5:
  v3. 0,'' \emph{Zenodo}, 2020.

\bibitem{li2022yolov6}
C.~Li, L.~Li, H.~Jiang, K.~Weng, Y.~Geng, L.~Li, Z.~Ke, Q.~Li, M.~Cheng, W.~Nie
  \emph{et~al.}, ``Yolov6: A single-stage object detection framework for
  industrial applications,'' \emph{arXiv preprint arXiv:2209.02976}, 2022.

\bibitem{wang2023yolov7}
C.-Y. Wang, A.~Bochkovskiy, and H.-Y.~M. Liao, ``Yolov7: Trainable
  bag-of-freebies sets new state-of-the-art for real-time object detectors,''
  in \emph{Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, 2023, pp. 7464--7475.

\bibitem{carion2020end}
N.~Carion, F.~Massa, G.~Synnaeve, N.~Usunier, A.~Kirillov, and S.~Zagoruyko,
  ``End-to-end object detection with transformers,'' in \emph{European
  conference on computer vision}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2020, pp. 213--229.

\bibitem{zhang2022dino}
H.~Zhang, F.~Li, S.~Liu, L.~Zhang, H.~Su, J.~Zhu, L.~M. Ni, and H.-Y. Shum,
  ``Dino: Detr with improved denoising anchor boxes for end-to-end object
  detection,'' \emph{arXiv preprint arXiv:2203.03605}, 2022.

\bibitem{li2023mask}
F.~Li, H.~Zhang, H.~Xu, S.~Liu, L.~Zhang, L.~M. Ni, and H.-Y. Shum, ``Mask
  dino: Towards a unified transformer-based framework for object detection and
  segmentation,'' in \emph{Proceedings of the IEEE/CVF conference on computer
  vision and pattern recognition}, 2023, pp. 3041--3050.

\bibitem{zhao2024detrs}
Y.~Zhao, W.~Lv, S.~Xu, J.~Wei, G.~Wang, Q.~Dang, Y.~Liu, and J.~Chen, ``Detrs
  beat yolos on real-time object detection,'' in \emph{Proceedings of the
  IEEE/CVF conference on computer vision and pattern recognition}, 2024, pp.
  16\,965--16\,974.

\bibitem{lv2024rt}
W.~Lv, Y.~Zhao, Q.~Chang, K.~Huang, G.~Wang, and Y.~Liu, ``Rt-detrv2: Improved
  baseline with bag-of-freebies for real-time detection transformer,''
  \emph{arXiv preprint arXiv:2407.17140}, 2024.

\bibitem{sohan2024review}
M.~Sohan, T.~Sai~Ram, and C.~V. Rami~Reddy, ``A review on yolov8 and its
  advancements,'' in \emph{International Conference on Data Intelligence and
  Cognitive Informatics}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
  2024, pp. 529--545.

\bibitem{wang2024yolov9}
C.-Y. Wang, I.-H. Yeh, and H.-Y. Mark~Liao, ``Yolov9: Learning what you want to
  learn using programmable gradient information,'' in \emph{European conference
  on computer vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2024,
  pp. 1--21.

\bibitem{wang2024yolov10}
A.~Wang, H.~Chen, L.~Liu, K.~Chen, Z.~Lin, J.~Han \emph{et~al.}, ``Yolov10:
  Real-time end-to-end object detection,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~37, pp. 107\,984--108\,011, 2024.

\bibitem{khanam2024yolov11}
R.~Khanam and M.~Hussain, ``Yolov11: An overview of the key architectural
  enhancements,'' \emph{arXiv preprint arXiv:2410.17725}, 2024.

\bibitem{tian2025yolov12}
Y.~Tian, Q.~Ye, and D.~Doermann, ``Yolov12: Attention-centric real-time object
  detectors,'' \emph{arXiv preprint arXiv:2502.12524}, 2025.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual
  models from natural language supervision,'' in \emph{International conference
  on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PmLR, 2021, pp.
  8748--8763.

\bibitem{gu2021open}
X.~Gu, T.-Y. Lin, W.~Kuo, and Y.~Cui, ``Open-vocabulary object detection via
  vision and language knowledge distillation,'' \emph{arXiv preprint
  arXiv:2104.13921}, 2021.

\bibitem{li2022grounded}
L.~H. Li, P.~Zhang, H.~Zhang, J.~Yang, C.~Li, Y.~Zhong, L.~Wang, L.~Yuan,
  L.~Zhang, J.-N. Hwang \emph{et~al.}, ``Grounded language-image
  pre-training,'' in \emph{Proceedings of the IEEE/CVF conference on computer
  vision and pattern recognition}, 2022, pp. 10\,965--10\,975.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin
  transformer: Hierarchical vision transformer using shifted windows,'' in
  \emph{Proceedings of the IEEE/CVF international conference on computer
  vision}, 2021, pp. 10\,012--10\,022.

\bibitem{zhang2022glipv2}
H.~Zhang, P.~Zhang, X.~Hu, Y.-C. Chen, L.~Li, X.~Dai, L.~Wang, L.~Yuan, J.-N.
  Hwang, and J.~Gao, ``Glipv2: Unifying localization and vision-language
  understanding,'' \emph{Advances in Neural Information Processing Systems},
  vol.~35, pp. 36\,067--36\,080, 2022.

\bibitem{liu2024yolo}
L.~Liu, J.~Feng, H.~Chen, A.~Wang, L.~Song, J.~Han, and G.~Ding, ``Yolo-uniow:
  Efficient universal open-world object detection,'' \emph{arXiv preprint
  arXiv:2412.20645}, 2024.

\bibitem{sun2025yolo}
Y.~Sun, J.~Wang, Y.~You, Z.~Yu, S.~Bian, E.~Wang, and W.~Wu, ``Yolo-e: a
  lightweight object detection algorithm for military targets,'' \emph{Signal,
  Image and Video Processing}, vol.~19, no.~3, p. 241, 2025.

\bibitem{pan2025locate}
J.~Pan, Y.~Liu, Y.~Fu, M.~Ma, J.~Li, D.~P. Paudel, L.~Van~Gool, and X.~Huang,
  ``Locate anything on earth: Advancing open-vocabulary object detection for
  remote sensing community,'' in \emph{Proceedings of the AAAI Conference on
  Artificial Intelligence}, vol.~39, no.~6, 2025, pp. 6281--6289.

\bibitem{huang2025openrsd}
Z.~Huang, Y.~Feng, S.~Yang, Z.~Liu, Q.~Liu, and Y.~Wang, ``Openrsd: Towards
  open-prompts for object detection in remote sensing images,'' \emph{arXiv
  preprint arXiv:2503.06146}, 2025.

\bibitem{lam2018xview}
D.~Lam, R.~Kuzma, K.~McGee, S.~Dooley, M.~Laielli, M.~Klaric, Y.~Bulatov, and
  B.~McCord, ``xview: Objects in context in overhead imagery,'' \emph{arXiv
  preprint arXiv:1802.07856}, 2018.

\bibitem{li2020object}
K.~Li, G.~Wan, G.~Cheng, L.~Meng, and J.~Han, ``Object detection in optical
  remote sensing images: A survey and a new benchmark,'' \emph{ISPRS journal of
  photogrammetry and remote sensing}, vol. 159, pp. 296--307, 2020.

\bibitem{isprs2018benchmark}
{ISPRS}, ``Benchmark on semantic labeling,'' Online:
  \url{https://www.isprs.org/education/benchmarks/UrbanSemLab/Default.aspx},
  2018, accessed: 2025-05-13.

\bibitem{wang2024mtp}
D.~Wang, J.~Zhang, M.~Xu, L.~Liu, D.~Wang, E.~Gao, C.~Han, H.~Guo, B.~Du,
  D.~Tao \emph{et~al.}, ``Mtp: Advancing remote sensing foundation model via
  multi-task pretraining,'' \emph{IEEE Journal of Selected Topics in Applied
  Earth Observations and Remote Sensing}, 2024.

\bibitem{ayush2021geography}
K.~Ayush, B.~Uzkent, C.~Meng, K.~Tanmay, M.~Burke, D.~Lobell, and S.~Ermon,
  ``Geography-aware self-supervised learning,'' in \emph{Proceedings of the
  IEEE/CVF International Conference on Computer Vision}, 2021, pp.
  10\,181--10\,190.

\bibitem{mall2023change}
U.~Mall, B.~Hariharan, and K.~Bala, ``Change-aware sampling and contrastive
  learning for satellite images,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2023, pp. 5261--5270.

\bibitem{tao2023tov}
C.~Tao, J.~Qi, G.~Zhang, Q.~Zhu, W.~Lu, and H.~Li, ``Tov: The original vision
  model for optical remote sensing image understanding via self-supervised
  learning,'' \emph{IEEE Journal of Selected Topics in Applied Earth
  Observations and Remote Sensing}, vol.~16, pp. 4916--4930, 2023.

\bibitem{reed2023scale}
C.~J. Reed, R.~Gupta, S.~Li, S.~Brockman, C.~Funk, B.~Clipp, K.~Keutzer,
  S.~Candido, M.~Uyttendaele, and T.~Darrell, ``Scale-mae: A scale-aware masked
  autoencoder for multiscale geospatial representation learning,'' in
  \emph{Proceedings of the IEEE/CVF International Conference on Computer
  Vision}, 2023, pp. 4088--4099.

\bibitem{bastani2023satlaspretrain}
F.~Bastani, P.~Wolters, R.~Gupta, J.~Ferdinando, and A.~Kembhavi,
  ``Satlaspretrain: A large-scale dataset for remote sensing image
  understanding,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2023, pp. 16\,772--16\,782.

\bibitem{sun2022ringmo}
X.~Sun, P.~Wang, W.~Lu, Z.~Zhu, X.~Lu, Q.~He, J.~Li, X.~Rong, Z.~Yang, H.~Chang
  \emph{et~al.}, ``Ringmo: A remote sensing foundation model with masked image
  modeling,'' \emph{IEEE Transactions on Geoscience and Remote Sensing},
  vol.~61, pp. 1--22, 2022.

\bibitem{guo2024skysense}
X.~Guo, J.~Lao, B.~Dang, Y.~Zhang, L.~Yu, L.~Ru, L.~Zhong, Z.~Huang, K.~Wu,
  D.~Hu \emph{et~al.}, ``Skysense: A multi-modal remote sensing foundation
  model towards universal interpretation for earth observation imagery,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2024, pp. 27\,672--27\,683.

\end{thebibliography}
