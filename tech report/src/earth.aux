\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{IEEEtran}
\citation{zareian2021open}
\citation{scheirer2012toward}
\citation{girshick2014rich}
\citation{liu2016ssd}
\citation{redmon2016you}
\citation{akyon2022slicing}
\citation{wu2022uiu}
\citation{yang2022querydet}
\citation{cheng2024yolo}
\citation{liu2024grounding}
\citation{ren2024grounding}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{girshick2014rich}
\citation{girshick2015fast}
\citation{ren2016faster}
\citation{he2017mask}
\citation{liu2016ssd}
\citation{redmon2016you}
\citation{redmon2017yolo9000}
\citation{redmon2018yolov3}
\citation{bochkovskiy2020yolov4}
\citation{jocher2020ultralytics}
\citation{li2022yolov6}
\citation{wang2023yolov7}
\citation{carion2020end}
\citation{zhang2022dino}
\citation{li2023mask}
\citation{zhao2024detrs}
\citation{lv2024rt}
\citation{sohan2024review}
\citation{wang2024yolov9}
\citation{wang2024yolov10}
\citation{khanam2024yolov11}
\citation{tian2025yolov12}
\citation{radford2021learning}
\citation{gu2021open}
\citation{li2022grounded}
\citation{liu2021swin}
\citation{zhang2022glipv2}
\citation{liu2024grounding}
\citation{cheng2024yolo}
\citation{liu2024yolo}
\citation{sun2025yolo}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Object Detection Models}{2}{subsection.2.1}\protected@file@percent }
\citation{pan2025locate}
\citation{huang2025openrsd}
\citation{lam2018xview}
\citation{li2020object}
\citation{isprs2018benchmark}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Open Vocabulary Detection Models}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Remote Sensing Image Open Vocabulary Detection}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Benchmark Datasets and Preprocessing}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Benchmark Dataset Selection}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Data Preprocessing}{4}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visualization of the data slicing process for the Potsdam dataset. The original high-resolution remote sensing images are divided into $224 \times 224$ pixel sub-images to facilitate training while preserving the detailed information within each tile.}}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:xView-slicing}{{1}{4}{Visualization of the data slicing process for the Potsdam dataset. The original high-resolution remote sensing images are divided into $224 \times 224$ pixel sub-images to facilitate training while preserving the detailed information within each tile}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of the data slicing process for the Potsdam dataset. The original high-resolution remote sensing images are divided into $224 \times 224$ pixel sub-images to facilitate training while preserving the detailed information within each tile.}}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig:potsdam-slicing}{{2}{5}{Visualization of the data slicing process for the Potsdam dataset. The original high-resolution remote sensing images are divided into $224 \times 224$ pixel sub-images to facilitate training while preserving the detailed information within each tile}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Construction of Lightweight Remote Sensing Open Vocabulary Detector}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experimental Workflow and Evaluation Metrics}{5}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Experimental workflow. The model is first pre-trained on xView, then evaluated cross-dataset transfer ability through zero-shot transfer and fine-tuning on DIOR, and finally tested for cross-task transfer on Potsdam.}}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:experimental-workflow}{{3}{6}{Experimental workflow. The model is first pre-trained on xView, then evaluated cross-dataset transfer ability through zero-shot transfer and fine-tuning on DIOR, and finally tested for cross-task transfer on Potsdam}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Model Design and Implementation}{7}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Model Architecture}{7}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The architecture of YOLO11-Earth. The model consists of a CSPNet backbone, a Vision-Language Path Aggregation Network (VL-PAN) for feature fusion, and a WorldDetect head for multi-modal detection. Text features from the CLIP text encoder are integrated at multiple levels of the network.}}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:yolo11-earth-architecture}{{4}{7}{The architecture of YOLO11-Earth. The model consists of a CSPNet backbone, a Vision-Language Path Aggregation Network (VL-PAN) for feature fusion, and a WorldDetect head for multi-modal detection. Text features from the CLIP text encoder are integrated at multiple levels of the network}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}VL-PAN Design for Closed Sets}{7}{subsubsection.4.2.2}\protected@file@percent }
\citation{wang2024mtp}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Multi-Modal workflow of YOLO11-Earth. The model aggregate the text embeddings into the image features at multiple levels. The text features are encoded once and passed through the network as a feature supplement to the traditional YOLO11 model.}}{8}{figure.caption.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Pretraining on Closed-set Datasets}{8}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Pretraining results of YOLO11-Earth and baseline models on xView-valid.}}{9}{figure.caption.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Metrics of YOLO11-Earth and baseline models on xView. In parentheses is the change of indicator relative to conf=default after conf=0.3 is used.}}{9}{table.caption.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Metrics of YOLO11-Earth and baseline models on the original-size xView validation set.}}{10}{table.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Cross-dataset Transfer Results}{10}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Zero-Shot Transfer Results}{10}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Full Fine-tuning Transfer Results}{10}{subsubsection.4.4.2}\protected@file@percent }
\citation{ayush2021geography}
\citation{mall2023change}
\citation{tao2023tov}
\citation{reed2023scale}
\citation{bastani2023satlaspretrain}
\citation{sun2022ringmo}
\citation{guo2024skysense}
\citation{wang2024mtp}
\citation{li2022grounded}
\citation{pan2025locate}
\citation{liu2024grounding}
\citation{pan2025locate}
\citation{liu2024grounding}
\citation{pan2025locate}
\citation{pan2025locate}
\citation{pan2025locate}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Metrics of YOLO11-Earth and baseline models on the DIOR test set for zero-shot transfer.}}{11}{table.caption.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Fine-tuning with full train set results of YOLO11-Earth and baseline models on DIOR.}}{11}{figure.caption.10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Metrics of YOLO11-Earth and baseline models on the DIOR validation and test sets after full fine-tuning.}}{12}{table.caption.11}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of YOLO11-Earth with state-of-the-art models on the DIOR test set.}}{12}{table.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Less Data Fine-tuning Transfer Results}{12}{subsubsection.4.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Model performance comparison with different training data sizes.}}{13}{table.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Cross-task Transfer Results}{13}{subsection.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Performance metrics of the YOLO11-seg model on the Potsdam dataset.}}{14}{table.caption.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Visualization of the segmentation results of the YOLO11x-Earth model on the Potsdam dataset.}}{14}{figure.caption.15}\protected@file@percent }
\newlabel{fig:segmentation-results}{{8}{14}{Visualization of the segmentation results of the YOLO11x-Earth model on the Potsdam dataset}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Cost-effectiveness and Interpretability}{14}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Cutting Down the Training Cost}{14}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Comparison of training costs for different open vocabulary detection models.}}{15}{table.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Text Embedding as Bias}{15}{subsection.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Visualization of the distribution shift caused by text features. (a) Original 6D input data reduced to 3D using PCA. (b) Output distribution before (purple) and after (red) adding text features, showing clear separation in the distribution spaces.}}{16}{figure.caption.17}\protected@file@percent }
\newlabel{fig:distribution-shift}{{9}{16}{Visualization of the distribution shift caused by text features. (a) Original 6D input data reduced to 3D using PCA. (b) Output distribution before (purple) and after (red) adding text features, showing clear separation in the distribution spaces}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3} Limitations}{16}{subsection.5.3}\protected@file@percent }
\bibdata{refs.bib}
\bibcite{zareian2021open}{1}
\bibcite{scheirer2012toward}{2}
\bibcite{girshick2014rich}{3}
\bibcite{liu2016ssd}{4}
\bibcite{redmon2016you}{5}
\bibcite{akyon2022slicing}{6}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{17}{section.6}\protected@file@percent }
\bibcite{wu2022uiu}{7}
\bibcite{yang2022querydet}{8}
\bibcite{cheng2024yolo}{9}
\bibcite{liu2024grounding}{10}
\bibcite{ren2024grounding}{11}
\bibcite{girshick2015fast}{12}
\bibcite{ren2016faster}{13}
\bibcite{he2017mask}{14}
\bibcite{redmon2017yolo9000}{15}
\bibcite{redmon2018yolov3}{16}
\bibcite{bochkovskiy2020yolov4}{17}
\bibcite{jocher2020ultralytics}{18}
\bibcite{li2022yolov6}{19}
\bibcite{wang2023yolov7}{20}
\bibcite{carion2020end}{21}
\bibcite{zhang2022dino}{22}
\bibcite{li2023mask}{23}
\bibcite{zhao2024detrs}{24}
\bibcite{lv2024rt}{25}
\bibcite{sohan2024review}{26}
\bibcite{wang2024yolov9}{27}
\bibcite{wang2024yolov10}{28}
\bibcite{khanam2024yolov11}{29}
\bibcite{tian2025yolov12}{30}
\bibcite{radford2021learning}{31}
\bibcite{gu2021open}{32}
\bibcite{li2022grounded}{33}
\bibcite{liu2021swin}{34}
\bibcite{zhang2022glipv2}{35}
\bibcite{liu2024yolo}{36}
\bibcite{sun2025yolo}{37}
\bibcite{pan2025locate}{38}
\bibcite{huang2025openrsd}{39}
\bibcite{lam2018xview}{40}
\bibcite{li2020object}{41}
\bibcite{isprs2018benchmark}{42}
\bibcite{wang2024mtp}{43}
\bibcite{ayush2021geography}{44}
\bibcite{mall2023change}{45}
\bibcite{tao2023tov}{46}
\bibcite{reed2023scale}{47}
\bibcite{bastani2023satlaspretrain}{48}
\bibcite{sun2022ringmo}{49}
\bibcite{guo2024skysense}{50}
\gdef \@abspage@last{20}
